\section{Нейросетевая регрессия как оценка апостериорной вероятности класса}

Рассмотрим гистограммные оценки плотности для двух классов:

\[
f_n(X) = \frac{n_1(X)}{n \cdot V(K_r)}, \quad p_n(X) = \frac{n_0(X)}{n \cdot V(K_r)},
\]

\noindent где \(V(K_r)\) -- мера ячейки \(K_r\).

На основе этих оценок можно определить байесовскую аппроксимацию апостериорной вероятности класса:
\[
h_n^*(X) = \frac{f_n(X)}{f_n(X) + p_n(X)}.
\]

Зададим порог \(\beta \in [0, 1)\). Тогда неравенство \(h_n^*(X) > \beta\) позволяет выделить из множества \(K\) ячейки с высоким значением выборочной оценки апостериорной вероятности класса \(+1\). Преобразуя выражение, получим эквивалентную форму~\cite{mitsobi2025}:
\[
f_n(X) > \frac{\beta}{1 - \beta} \cdot p_n(X).
\]

Иными словами, объект \(X\) попадает в область пространства признаков, где плотность первого класса превышает плотность фонового класса более чем в \(\frac{\beta}{1 - \beta}\) раз. Это условие можно использовать для выделения из множества ячеек \(\{K_1, K_2, \ldots, K_N\}\) подмножества, соответствующего кластеру высокой плотности. Такие области могут рассматриваться как выборочные приближения к области, поддерживающей плотность распределения первого класса.

По аналогии с разделом~\cref{sec:ch1/hist_neural_approximations}, согласно результатам работы~\cite{devroye2013probabilistic}, для обеспечения статистической состоятельности гистограммных оценок плотности необходимо, чтобы с ростом объёма обучающей выборки происходило соответствующее увеличение числа нейронов в аппроксимирующей сети. Это условие отражает потребность в возрастающем разрешении пространства признаков, необходимом для точного приближения апостериорной вероятности.

В таком случае для решения о принадлежности нового наблюдения \(X\) области высокой плотности достаточно проверить выполнение неравенства \(c_n^*(X) > \beta\), что представляет собой гораздо более простую с вычислительной точки зрения операцию, чем вычисление \(h_n^*(X)\).

Таким образом, задача принятия решения сводится к сравнению выхода нейросети с порогом, что обеспечивает линейную по числу слоёв и нейронов вычислительную сложность и устраняет необходимость работы с явным представлением плотностей. Это делает метод особенно привлекательным в задачах, требующих масштабируемости и эффективности при обработке новых входных данных.
